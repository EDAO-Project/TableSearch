{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tables_dir='../wikitables/files/wikitables_raw/'\n",
    "tables_output_dir='wikipages_expanded_dataset_strubert_format/tables/'\n",
    "queries_output_dir='wikipages_expanded_dataset_strubert_format/queries/'\n",
    "input_tables_dir='wikipages_expanded_dataset/tables/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tables=set(os.listdir(input_tables_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_cell_value_fields(table_dict):\n",
    "    '''\n",
    "    Given the dictionary of a table (as presented in the raw json files) replace all non-data values\n",
    "    with the empty \"\" string\n",
    "    '''\n",
    "    table_dict['title'] = [\"\"] * len(table_dict['title'])\n",
    "    table_dict['pgTitle'] = \"\"\n",
    "    table_dict['secondTitle'] = \"\"\n",
    "    table_dict['caption'] = \"\"\n",
    "\n",
    "    return table_dict\n",
    "\n",
    "def select_data_rows(table_dict, selected_row_ids):\n",
    "    '''\n",
    "    Given the dictionary of a table (as presented in the raw json files) select only the specified `selected_row_ids`\n",
    "    from its data field.\n",
    "    '''\n",
    "    table_dict['data'] = [table_dict['data'][i] for i in selected_row_ids]\n",
    "    return table_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653/1653 [02:54<00:00,  9.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop over all raw tables and select the ones in the `input_tables` set.\n",
    "# The selected tables are saved in the output_dir\n",
    "for filename in tqdm(sorted(os.listdir(raw_tables_dir))):\n",
    "    with open(raw_tables_dir+filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filtered_dict = dict()\n",
    "\n",
    "    for table in data:\n",
    "        if table +'.json' in input_tables:\n",
    "            table_dict = data[table]\n",
    "            # Set non-cell value fields to the empty \"\" string\n",
    "            table_dict = remove_non_cell_value_fields(table_dict)\n",
    "            filtered_dict[table] = table_dict\n",
    "    \n",
    "    # Check if the filtered dict is not empty and save it to the `tables_output_dir`\n",
    "    if len(filtered_dict) > 0:\n",
    "        with open(tables_output_dir + filename, 'w') as fp:\n",
    "            json.dump(filtered_dict, fp,  indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1650/1650 [00:16<00:00, 98.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 238038 unique tables under the wikipages_expanded_dataset_strubert_format/tables/ directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity check. Check how many unique tables there are in total under the `tables_output_dir`\n",
    "unique_tables = set()\n",
    "for filename in tqdm(os.listdir(tables_output_dir)):\n",
    "    with open(tables_output_dir+filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for table_name in data:\n",
    "        unique_tables.add(table_name)\n",
    "print('There are in total', len(unique_tables), 'unique tables under the', tables_output_dir, 'directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikipage</th>\n",
       "      <th>wikipage_id</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>tables</th>\n",
       "      <th>num_entities</th>\n",
       "      <th>tuple_width</th>\n",
       "      <th>num_tuples</th>\n",
       "      <th>selected_table</th>\n",
       "      <th>selected_row_ids</th>\n",
       "      <th>categories_relevant_wikipages</th>\n",
       "      <th>categories_relevant_tables</th>\n",
       "      <th>navigation_links_relevant_wikipages</th>\n",
       "      <th>navigation_links_relevant_tables</th>\n",
       "      <th>categories_expansion_ratio</th>\n",
       "      <th>navigation_links_expansion_ratio</th>\n",
       "      <th>avg_query_containment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Andre_Norton_Award</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-0001-242.json]</td>\n",
       "      <td>[80]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>table-0001-242.json</td>\n",
       "      <td>[4, 6, 7, 10, 13, 14, 28, 30, 31, 33, 36]</td>\n",
       "      <td>51.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>https://en.wikipedia.org/wiki/President_of_Ind...</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-0001-319.json]</td>\n",
       "      <td>[20]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>table-0001-319.json</td>\n",
       "      <td>[0, 3, 4, 5, 6, 7, 8, 12, 14, 18]</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.271930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Charlotte_Bobcat...</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-0001-460.json]</td>\n",
       "      <td>[108]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>table-0001-460.json</td>\n",
       "      <td>[0, 8, 9, 15, 19, 21, 26, 28, 31, 34, 38, 39, ...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_organism...</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-0001-469.json]</td>\n",
       "      <td>[355]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>table-0001-469.json</td>\n",
       "      <td>[2, 44, 45, 53, 80, 107, 206, 208, 213, 219, 2...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>https://en.wikipedia.org/wiki/1982_NCAA_Women'...</td>\n",
       "      <td>97</td>\n",
       "      <td>2</td>\n",
       "      <td>[table-0001-64.json, table-0001-65.json]</td>\n",
       "      <td>[12, 58]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>table-0001-65.json</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15]</td>\n",
       "      <td>34.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244164</th>\n",
       "      <td>https://en.wikipedia.org/wiki/2004_LPGA_Tour</td>\n",
       "      <td>244164</td>\n",
       "      <td>2</td>\n",
       "      <td>[table-1653-355.json, table-1653-356.json]</td>\n",
       "      <td>[64, 15]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>table-1653-355.json</td>\n",
       "      <td>[4, 6, 7, 8, 10, 16, 17, 24, 26, 30]</td>\n",
       "      <td>68.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244174</th>\n",
       "      <td>https://en.wikipedia.org/wiki/1998_NCAA_Women'...</td>\n",
       "      <td>244174</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-1653-409.json]</td>\n",
       "      <td>[56]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>table-1653-409.json</td>\n",
       "      <td>[0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 15]</td>\n",
       "      <td>40.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.206687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244209</th>\n",
       "      <td>https://en.wikipedia.org/wiki/1931_Italian_Gra...</td>\n",
       "      <td>244209</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-1653-615.json]</td>\n",
       "      <td>[26]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>table-1653-615.json</td>\n",
       "      <td>[0, 1, 2, 3, 4, 11, 15, 19, 21, 22, 23, 24, 27]</td>\n",
       "      <td>88.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244229</th>\n",
       "      <td>https://en.wikipedia.org/wiki/List_of_World_Aq...</td>\n",
       "      <td>244229</td>\n",
       "      <td>16</td>\n",
       "      <td>[table-1653-718.json, table-1653-720.json, tab...</td>\n",
       "      <td>[30, 44, 45, 40, 40, 44, 41, 24, 21, 47, 25, 4...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>table-1653-730.json</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.131381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244234</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Australian_feder...</td>\n",
       "      <td>244234</td>\n",
       "      <td>1</td>\n",
       "      <td>[table-1653-759.json]</td>\n",
       "      <td>[43]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>table-1653-759.json</td>\n",
       "      <td>[3, 4, 7, 9, 10, 11, 13, 14, 16, 17]</td>\n",
       "      <td>43.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9771 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 wikipage  wikipage_id  \\\n",
       "32       https://en.wikipedia.org/wiki/Andre_Norton_Award           32   \n",
       "46      https://en.wikipedia.org/wiki/President_of_Ind...           46   \n",
       "66      https://en.wikipedia.org/wiki/Charlotte_Bobcat...           66   \n",
       "68      https://en.wikipedia.org/wiki/List_of_organism...           68   \n",
       "97      https://en.wikipedia.org/wiki/1982_NCAA_Women'...           97   \n",
       "...                                                   ...          ...   \n",
       "244164       https://en.wikipedia.org/wiki/2004_LPGA_Tour       244164   \n",
       "244174  https://en.wikipedia.org/wiki/1998_NCAA_Women'...       244174   \n",
       "244209  https://en.wikipedia.org/wiki/1931_Italian_Gra...       244209   \n",
       "244229  https://en.wikipedia.org/wiki/List_of_World_Aq...       244229   \n",
       "244234  https://en.wikipedia.org/wiki/Australian_feder...       244234   \n",
       "\n",
       "        num_tables                                             tables  \\\n",
       "32               1                              [table-0001-242.json]   \n",
       "46               1                              [table-0001-319.json]   \n",
       "66               1                              [table-0001-460.json]   \n",
       "68               1                              [table-0001-469.json]   \n",
       "97               2           [table-0001-64.json, table-0001-65.json]   \n",
       "...            ...                                                ...   \n",
       "244164           2         [table-1653-355.json, table-1653-356.json]   \n",
       "244174           1                              [table-1653-409.json]   \n",
       "244209           1                              [table-1653-615.json]   \n",
       "244229          16  [table-1653-718.json, table-1653-720.json, tab...   \n",
       "244234           1                              [table-1653-759.json]   \n",
       "\n",
       "                                             num_entities  tuple_width  \\\n",
       "32                                                   [80]          4.0   \n",
       "46                                                   [20]          3.0   \n",
       "66                                                  [108]          3.0   \n",
       "68                                                  [355]          3.0   \n",
       "97                                               [12, 58]          4.0   \n",
       "...                                                   ...          ...   \n",
       "244164                                           [64, 15]          3.0   \n",
       "244174                                               [56]          4.0   \n",
       "244209                                               [26]          3.0   \n",
       "244229  [30, 44, 45, 40, 40, 44, 41, 24, 21, 47, 25, 4...          4.0   \n",
       "244234                                               [43]          3.0   \n",
       "\n",
       "        num_tuples       selected_table  \\\n",
       "32            11.0  table-0001-242.json   \n",
       "46            10.0  table-0001-319.json   \n",
       "66            28.0  table-0001-460.json   \n",
       "68            14.0  table-0001-469.json   \n",
       "97            13.0   table-0001-65.json   \n",
       "...            ...                  ...   \n",
       "244164        10.0  table-1653-355.json   \n",
       "244174        11.0  table-1653-409.json   \n",
       "244209        13.0  table-1653-615.json   \n",
       "244229        15.0  table-1653-730.json   \n",
       "244234        10.0  table-1653-759.json   \n",
       "\n",
       "                                         selected_row_ids  \\\n",
       "32              [4, 6, 7, 10, 13, 14, 28, 30, 31, 33, 36]   \n",
       "46                      [0, 3, 4, 5, 6, 7, 8, 12, 14, 18]   \n",
       "66      [0, 8, 9, 15, 19, 21, 26, 28, 31, 34, 38, 39, ...   \n",
       "68      [2, 44, 45, 53, 80, 107, 206, 208, 213, 219, 2...   \n",
       "97            [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 13, 14, 15]   \n",
       "...                                                   ...   \n",
       "244164               [4, 6, 7, 8, 10, 16, 17, 24, 26, 30]   \n",
       "244174               [0, 1, 2, 3, 4, 7, 8, 9, 10, 11, 15]   \n",
       "244209    [0, 1, 2, 3, 4, 11, 15, 19, 21, 22, 23, 24, 27]   \n",
       "244229  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "244234               [3, 4, 7, 9, 10, 11, 13, 14, 16, 17]   \n",
       "\n",
       "        categories_relevant_wikipages  categories_relevant_tables  \\\n",
       "32                               51.0                        58.0   \n",
       "46                                7.0                         7.0   \n",
       "66                               27.0                       122.0   \n",
       "68                               16.0                        37.0   \n",
       "97                               34.0                        39.0   \n",
       "...                               ...                         ...   \n",
       "244164                           68.0                        81.0   \n",
       "244174                           40.0                        48.0   \n",
       "244209                           88.0                       129.0   \n",
       "244229                           44.0                       240.0   \n",
       "244234                           43.0                        67.0   \n",
       "\n",
       "        navigation_links_relevant_wikipages  navigation_links_relevant_tables  \\\n",
       "32                                      NaN                               NaN   \n",
       "46                                      NaN                               NaN   \n",
       "66                                      NaN                               NaN   \n",
       "68                                      NaN                               NaN   \n",
       "97                                      NaN                               NaN   \n",
       "...                                     ...                               ...   \n",
       "244164                                  NaN                               NaN   \n",
       "244174                                  NaN                               NaN   \n",
       "244209                                  NaN                               NaN   \n",
       "244229                                  NaN                               NaN   \n",
       "244234                                  NaN                               NaN   \n",
       "\n",
       "        categories_expansion_ratio  navigation_links_expansion_ratio  \\\n",
       "32                            58.0                               NaN   \n",
       "46                             7.0                               NaN   \n",
       "66                           122.0                               NaN   \n",
       "68                            37.0                               NaN   \n",
       "97                            19.5                               NaN   \n",
       "...                            ...                               ...   \n",
       "244164                        40.5                               NaN   \n",
       "244174                        48.0                               NaN   \n",
       "244209                       129.0                               NaN   \n",
       "244229                        15.0                               NaN   \n",
       "244234                        67.0                               NaN   \n",
       "\n",
       "        avg_query_containment  \n",
       "32                   0.036120  \n",
       "46                   0.271930  \n",
       "66                   0.026968  \n",
       "68                   0.002137  \n",
       "97                   0.210526  \n",
       "...                       ...  \n",
       "244164               0.237500  \n",
       "244174               0.206687  \n",
       "244209               0.075247  \n",
       "244229               0.131381  \n",
       "244234               0.030808  \n",
       "\n",
       "[9771 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df = pd.read_pickle('../../queries/wikipages/query_dataframes/expanded_wikipages/filtered_queries/minTupleWidth_all_tuplesPerQuery_all.pickle')\n",
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653/1653 [02:40<00:00, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "query_tables = set(queries_df['selected_table'])\n",
    "\n",
    "num_tuples_per_query_list = [1,2,5,10]\n",
    "\n",
    "# Loop over all raw tables and select the ones in the `query_tables` set.\n",
    "# The selected tables are saved in the output_dir\n",
    "for filename in tqdm(sorted(os.listdir(raw_tables_dir))):\n",
    "    with open(raw_tables_dir+filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dict_of_filtered_dict = dict()\n",
    "    dict_of_filtered_dict['full'] = dict()\n",
    "    dict_of_filtered_dict['1_rows_per_query'] = dict()\n",
    "    dict_of_filtered_dict['2_rows_per_query'] = dict()\n",
    "    dict_of_filtered_dict['5_rows_per_query'] = dict()\n",
    "    dict_of_filtered_dict['10_rows_per_query'] = dict()\n",
    "\n",
    "    for table in data:\n",
    "        if table +'.json' in query_tables:\n",
    "            table_dict = data[table]\n",
    "            # Set non-cell value fields to the empty \"\" string\n",
    "            table_dict = remove_non_cell_value_fields(table_dict)\n",
    "\n",
    "            # Get the selected row IDS\n",
    "            selected_row_ids = queries_df[queries_df['selected_table']==table+'.json']['selected_row_ids'].tolist()[0]\n",
    "            full_table_dict = select_data_rows(table_dict, selected_row_ids)\n",
    "            dict_of_filtered_dict['full'][table] = full_table_dict\n",
    "\n",
    "            # Update the table_dict by choosing variable sized tuples per query\n",
    "            for tuple_size in num_tuples_per_query_list:\n",
    "                tmp_table_dict = full_table_dict.copy()\n",
    "                tmp_table_dict['data'] = tmp_table_dict['data'][:tuple_size]\n",
    "                dict_of_filtered_dict[str(tuple_size) + '_rows_per_query'][table] = tmp_table_dict\n",
    "\n",
    "  \n",
    "    # Check if the filtered dict is not empty and save it to the `queries_output_dir`\n",
    "    if len(dict_of_filtered_dict['full']) > 0:\n",
    "        with open(queries_output_dir + 'full/' + filename, 'w') as fp:\n",
    "            json.dump(dict_of_filtered_dict['full'], fp,  indent=2)\n",
    "\n",
    "        # Save the variable number of tuples per query files\n",
    "        for tuple_size in num_tuples_per_query_list:\n",
    "            with open(queries_output_dir + str(tuple_size) + '_rows_per_query/' + filename, 'w') as fp:\n",
    "                json.dump(dict_of_filtered_dict[str(tuple_size) + '_rows_per_query'], fp,  indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the query.txt file\n",
    "query_txt_output_dir='wikipages_expanded_dataset_strubert_format/queries/'\n",
    "query_file_dict={'query_id': [], 'query_table': []}\n",
    "for idx, row in queries_df.iterrows():\n",
    "    query_file_dict['query_id'].append(row['wikipage_id'])\n",
    "    query_file_dict['query_table'].append(os.path.splitext(row['selected_table'])[0])\n",
    "\n",
    "query_file_df = pd.DataFrame.from_dict(query_file_dict)\n",
    "query_file_df.to_csv(query_txt_output_dir+'query.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1653/1653 [02:28<00:00, 11.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform the same procedure but only over the sampled queries (i.e., copy files only for the sampled queries)\n",
    "sampled_queries_output_dir = 'wikipages_expanded_dataset_strubert_format/sampled_queries/'\n",
    "sampled_queries_df = pd.read_pickle('../../queries/wikipages/query_dataframes/expanded_wikipages/sampled_queries/sampled_queries.pickle')\n",
    "query_tables = set(sampled_queries_df['selected_table'])\n",
    "\n",
    "num_tuples_per_query_list = [1,2,5,10]\n",
    "\n",
    "# Loop over all raw tables and select the ones in the `query_tables` set.\n",
    "# The selected tables are saved in the output_dir\n",
    "for filename in tqdm(sorted(os.listdir(raw_tables_dir))):\n",
    "    with open(raw_tables_dir+filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dict_of_filtered_dict = dict()\n",
    "    dict_of_filtered_dict['full'] = dict()\n",
    "    dict_of_filtered_dict['1_rows_per_query'] = dict()\n",
    "    dict_of_filtered_dict['2_rows_per_query'] = dict()\n",
    "    dict_of_filtered_dict['5_rows_per_query'] = dict()\n",
    "    dict_of_filtered_dict['10_rows_per_query'] = dict()\n",
    "\n",
    "    for table in data:\n",
    "        if table +'.json' in query_tables:\n",
    "            table_dict = data[table]\n",
    "            # Set non-cell value fields to the empty \"\" string\n",
    "            table_dict = remove_non_cell_value_fields(table_dict)\n",
    "\n",
    "            # Get the selected row IDS\n",
    "            selected_row_ids = sampled_queries_df[sampled_queries_df['selected_table']==table+'.json']['selected_row_ids'].tolist()[0]\n",
    "            full_table_dict = select_data_rows(table_dict, selected_row_ids)\n",
    "            dict_of_filtered_dict['full'][table] = full_table_dict\n",
    "\n",
    "            # Update the table_dict by choosing variable sized tuples per query\n",
    "            for tuple_size in num_tuples_per_query_list:\n",
    "                tmp_table_dict = full_table_dict.copy()\n",
    "                tmp_table_dict['data'] = tmp_table_dict['data'][:tuple_size]\n",
    "                dict_of_filtered_dict[str(tuple_size) + '_rows_per_query'][table] = tmp_table_dict\n",
    "\n",
    "  \n",
    "    # Check if the filtered dict is not empty and save it to the `sampled_queries_output_dir`\n",
    "    if len(dict_of_filtered_dict['full']) > 0:\n",
    "        with open(sampled_queries_output_dir + 'full/' + filename, 'w') as fp:\n",
    "            json.dump(dict_of_filtered_dict['full'], fp,  indent=2)\n",
    "\n",
    "        # Save the variable number of tuples per query files\n",
    "        for tuple_size in num_tuples_per_query_list:\n",
    "            with open(sampled_queries_output_dir + str(tuple_size) + '_rows_per_query/' + filename, 'w') as fp:\n",
    "                json.dump(dict_of_filtered_dict[str(tuple_size) + '_rows_per_query'], fp,  indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the query.txt file\n",
    "query_txt_output_dir='wikipages_expanded_dataset_strubert_format/sampled_queries/'\n",
    "query_file_dict={'query_id': [], 'query_table': []}\n",
    "for idx, row in sampled_queries_df.iterrows():\n",
    "    query_file_dict['query_id'].append(row['wikipage_id'])\n",
    "    query_file_dict['query_table'].append(os.path.splitext(row['selected_table'])[0])\n",
    "\n",
    "query_file_df = pd.DataFrame.from_dict(query_file_dict)\n",
    "query_file_df.to_csv(query_txt_output_dir+'query.txt', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_wikipages(wikipage_id, relevance_scores_dir):\n",
    "    '''\n",
    "    Given a wikipage_id return a dictionary keyed by the relevant wikipages and their relevance scores\n",
    "    '''\n",
    "    with open(relevance_scores_dir + str(wikipage_id) + '.json') as fp:\n",
    "        relevant_wikipages_dict = json.load(fp)\n",
    "    return relevant_wikipages_dict\n",
    "\n",
    "def get_relevant_wikitables(relevant_wikipages, df):\n",
    "    '''\n",
    "    Given a dictionary of the relevant wikipages and their relevance scores return a dictionary\n",
    "    of the relevant wikitables and their relevance scores\n",
    "    '''\n",
    "    relevant_wikitables_dict = dict()\n",
    "    for wikipage in relevant_wikipages:\n",
    "        wikipage_link = 'https://en.wikipedia.org/wiki/' + wikipage\n",
    "        if wikipage_link in df['wikipage'].values:\n",
    "            wikitables = df[df['wikipage']==wikipage_link]['tables'].values[0]\n",
    "            for wikitable in wikitables:\n",
    "                relevant_wikitables_dict[wikitable] = relevant_wikipages[wikipage]\n",
    "  \n",
    "    return relevant_wikitables_dict\n",
    "\n",
    "def get_groundtruth_df(query_id, relevant_wikitables_dict, search_space_tables):\n",
    "    '''\n",
    "    Returns a dataframe of the groundtruth relevance for the specified `query_id` in the format\n",
    "    used by StruBERT\n",
    "    '''\n",
    "\n",
    "    df_dict = {'query_id': [], 'dummy_var': [], 'table': [], 'relevance_score': []}\n",
    "    for table in relevant_wikitables_dict:\n",
    "        df_dict['query_id'].append(query_id)\n",
    "        df_dict['dummy_var'].append(0)\n",
    "        df_dict['table'].append(os.path.splitext(table)[0])\n",
    "        df_dict['relevance_score'].append(relevant_wikitables_dict[table])\n",
    "\n",
    "    # relevant_tables = set([os.path.splitext(table)[0] for table in relevant_wikitables_dict.keys()])\n",
    "    # non_relevant_tables = search_space_tables - relevant_tables\n",
    "    # for table in non_relevant_tables:\n",
    "    #     df_dict['query_id'].append(query_id)\n",
    "    #     df_dict['dummy_var'].append(0)\n",
    "    #     df_dict['table'].append(table)\n",
    "    #     df_dict['relevance_score'].append(0)\n",
    "\n",
    "    gt_df = pd.DataFrame.from_dict(df_dict)\n",
    "    return gt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('wikipages_expanded_dataset/wikipages_df.pickle')\n",
    "queries_df = pd.read_pickle('../../queries/wikipages/query_dataframes/expanded_wikipages/filtered_queries/minTupleWidth_all_tuplesPerQuery_all.pickle')\n",
    "categories_relevance_scores_dir = '../../queries/wikipages/groundtruth_generation/wikipage_relevance_scores/wikipages_expanded_dataset/jaccard_categories_new/'\n",
    "groundtruth_output_dir='wikipages_expanded_dataset_strubert_format/groundtruth_query_relevance/only_relevant/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1650/1650 [00:15<00:00, 104.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 238038 tables in our search space.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# The set of tables in our search space\n",
    "search_space_tables = set()\n",
    "for filename in tqdm(os.listdir(tables_output_dir)):\n",
    "    with open(tables_output_dir+filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for table_name in data:\n",
    "        search_space_tables.add(table_name)\n",
    "print(\"There are\", len(search_space_tables), 'tables in our search space.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in tqdm(queries_df.iterrows(), total=queries_df.shape[0]):\n",
    "    wikipage_id=row['wikipage_id']\n",
    "    query_table=os.path.splitext(row['selected_table'])[0]\n",
    "    relevant_wikipages_dict = get_relevant_wikipages(wikipage_id=wikipage_id, relevance_scores_dir=categories_relevance_scores_dir)\n",
    "    relevant_wikitables_dict = get_relevant_wikitables(relevant_wikipages=relevant_wikipages_dict, df=df)\n",
    "    \n",
    "    gt_df = get_groundtruth_df(query_id=wikipage_id, relevant_wikitables_dict=relevant_wikitables_dict, search_space_tables=search_space_tables)\n",
    "\n",
    "    # Save the gt_df into a .tsv file\n",
    "    gt_df.to_csv(groundtruth_output_dir+query_table+'.tsv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "df = pd.read_pickle('wikipages_expanded_dataset/wikipages_df.pickle')\n",
    "sampled_queries_df = pd.read_pickle('../../queries/wikipages/query_dataframes/expanded_wikipages/sampled_queries/sampled_queries.pickle')\n",
    "groundtruth_output_dir='wikipages_expanded_dataset_strubert_format/groundtruth_query_relevance/'\n",
    "sampled_queries_groundtruth_output_dir='wikipages_expanded_dataset_strubert_format/sampled_queries_groundtruth_query_relevance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:24<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Copy the .tsv files for each query table in the sampled_queries_df\n",
    "for idx, row in tqdm(sampled_queries_df.iterrows(), total=sampled_queries_df.shape[0]):\n",
    "    table_name = os.path.splitext(row['selected_table'])[0]\n",
    "\n",
    "    # Copy full groundtruth\n",
    "    shutil.copy(groundtruth_output_dir+'full/'+table_name+'.tsv', sampled_queries_groundtruth_output_dir+'full/')\n",
    "\n",
    "    # Copy only relevant groundtruth\n",
    "    shutil.copy(groundtruth_output_dir+'only_relevant/'+table_name+'.tsv', sampled_queries_groundtruth_output_dir+'only_relevant/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:33<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a shortened groundtruth for the sampled queries. The shortened groundtruth is composed by selecting all non-zero relevant tables for a query\n",
    "# and adding randomly N non-relevant tables where N is equal tot he number of non-zero relevant tables\n",
    "for filename in tqdm(sorted(os.listdir(sampled_queries_groundtruth_output_dir+'full/'))):\n",
    "    full_gt_df = pd.read_csv(sampled_queries_groundtruth_output_dir + 'full/' + filename, sep='\\t', names=['query_id', 'dummy_var', 'table', 'relevance_score'])\n",
    "\n",
    "    short_gt_df = full_gt_df[full_gt_df['relevance_score'] > 0]\n",
    "    \n",
    "    # Randomly select len(short_gt_df) zero relevance rows from full_gt_df\n",
    "    zero_relevance_rows_df = full_gt_df[full_gt_df['relevance_score'] == 0]\n",
    "    sampled_rows = zero_relevance_rows_df.sample(n=len(short_gt_df), random_state=1)\n",
    "\n",
    "    # Combine the short_gt_df with the sampled_rows    \n",
    "    short_gt_df = pd.concat([short_gt_df, sampled_rows])\n",
    "\n",
    "    # Save the dataframe\n",
    "    short_gt_df.to_csv(sampled_queries_groundtruth_output_dir + 'shortened_gt/' + filename, index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create k-fold cross validation splits for the sampled queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 120.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "shortened_gt_dir = 'wikipages_expanded_dataset_strubert_format/sampled_queries_groundtruth_query_relevance/shortened_gt/'\n",
    "k_folds_output_dir = 'wikipages_expanded_dataset_strubert_format/sampled_queries_groundtruth_query_relevance/k_folds/'\n",
    "\n",
    "# Dictionaries keyed by the fold number and mapping to the list of dataframes (one for each filename)\n",
    "train_dfs_dict = {1: [], 2: [], 3: [], 4: [], 5: []}\n",
    "test_dfs_dict = {1: [], 2: [], 3: [], 4: [], 5: []}\n",
    "\n",
    "# Loop over each file in the `shortened_gt_dir` and construct \n",
    "for filename in tqdm(sorted(os.listdir(shortened_gt_dir))):\n",
    "    gt_df = pd.read_csv(shortened_gt_dir + filename, sep='\\t', names=['query_id', 'dummy_var', 'table', 'relevance_score'])\n",
    "\n",
    "    non_zero_rel_df = gt_df[gt_df['relevance_score']>0] \n",
    "    zero_rel_df = gt_df[gt_df['relevance_score']==0] \n",
    "\n",
    "    train_non_zero_rel_splits=[]\n",
    "    test_non_zero_rel_splits=[]\n",
    "\n",
    "    train_zero_rel_splits=[]\n",
    "    test_zero_rel_splits=[]\n",
    "    \n",
    "    kf = KFold(n_splits = 5, shuffle = True, random_state = 1)   \n",
    "    # Folds over the non-zero relevance tables\n",
    "    for train, test in kf.split(non_zero_rel_df):\n",
    "        train_non_zero_rel_splits.append(non_zero_rel_df.iloc[train])\n",
    "        test_non_zero_rel_splits.append(non_zero_rel_df.iloc[test])\n",
    "\n",
    "    # Folds over zero-relevance tables\n",
    "    for train, test in kf.split(zero_rel_df):\n",
    "        train_zero_rel_splits.append(zero_rel_df.iloc[train])\n",
    "        test_zero_rel_splits.append(zero_rel_df.iloc[test])\n",
    "\n",
    "    # Concatenate the zero and non-zero relevance test and train splits into a single dataframe\n",
    "    for i in range(len(train_non_zero_rel_splits)):\n",
    "        train_dfs_dict[i+1].append(pd.concat([train_non_zero_rel_splits[i], train_zero_rel_splits[i]]))\n",
    "        test_dfs_dict[i+1].append(pd.concat([test_non_zero_rel_splits[i], test_zero_rel_splits[i]]))\n",
    "\n",
    "# Save the folds by concatenating the list of dataframes\n",
    "for i in range(1, 6):\n",
    "    df_train = pd.concat(train_dfs_dict[i])\n",
    "    df_test = pd.concat(test_dfs_dict[i])\n",
    "\n",
    "    df_train.to_csv(k_folds_output_dir + 'train_fold_' + str(i) + '.tsv', index=False, header=False, sep='\\t')\n",
    "    df_test.to_csv(k_folds_output_dir + 'test_fold_' + str(i) + '.tsv', index=False, header=False, sep='\\t')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e73418e12d8776f50940d995982e5df3d8bc3820d760e67e862468f908015cd9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
